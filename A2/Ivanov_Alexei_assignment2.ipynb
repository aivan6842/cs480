{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PMvDlW7YuXD4"
      },
      "source": [
        "# CS480/680, Spring 2023, Assignment 2\n",
        "## Designer: Yimu Wang; Instructor: Hongyang Zhang\n",
        "## Released: June 5; Due: June 25, noon\n",
        "\n",
        "[Link of the Assignment](https://colab.research.google.com/drive/1WjC7sUNiBRPga7M011Qope7lDuKaxNzc#scrollTo=PMvDlW7YuXD4)\n",
        "\n",
        "\n",
        "Tips:\n",
        "- Please save a copy of this notebook to avoid losing your changes.\n",
        "- Debug your code and ensure that it can run.\n",
        "- Save the output of each cell. Failure to do so may result in your coding questions not being graded.\n",
        "- **To accelerate the training time, you can choose 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' and set 'Hardware accelerator' to 'GPU'. With T4, all the experiments can be done in 5 minutes. In total, this notebook can be run in 20 minutes.** (You can either use or not use GPU to accelearte. It is your choice and it does not affect your grades.)\n",
        "- Your grade is independent of the test accuracy (unless it is 10%, as 10% is the accuracy of random guess).\n",
        "\n",
        "Tips for sumbission:\n",
        "- Do not change the order of the problems.\n",
        "- Select 'Runtime' -> 'Run all' to run all cells and generate a final \"gradable\" version of your notebook and save your ipynb file.\n",
        "- **We recommend using Chrome to generate the PDF.**\n",
        "- Also use 'File' -> 'Print' and then print your report from your browser into a PDF file.\n",
        "- **Submit both the .pdf and .ipynb files.**\n",
        "- **We do not accept any hand-written report.**\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AEVYyg3-FPMj"
      },
      "source": [
        "## Question 0. **[Important] Please name your submission as {Last-name}\\_{First-name}\\_{assignment2}.ipynb and {Last-name}\\_{First-name}\\_{assignment2}.pdf. If you do not follow this rule, your grade of assignment 2 will be 0.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xhdVaFciIPvV"
      },
      "source": [
        "## Question 1. Basics of MLP, CNN, and Transformers (40 points)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M1TwKbq7IPvV"
      },
      "source": [
        "### 1.1 Given an MLP with an input layer of size 100, two hidden layers of size 50 and 25, and an output layer of size 10, calculate the total number of parameters in the network. (10 points)\n",
        "\n",
        "This MLP network is a standard MLP with bias terms. The activation function is ReLU. But you do not need to consider the parameters of the activation function.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D6e9OkDHIPvW"
      },
      "source": [
        "Solution:\n",
        "[Your answer here, You should give the full calculation process.]\n",
        "\n",
        "Conceptually the weights lie in between layers. Since the input layer has 100 nodes and each of the 100 nodes has connections to the 50 nodes in the first hidden layer, we can represent the connections in a $\\mathbb{R}^{100 \\times 50}$ matrix. For each node in the first hidden layer, there is a bias term which can be represented in a $\\mathbb{R}^{50}$ matrix. The first hidden layer has 50 nodes, each of which connect to the 25 nodes of the second hidden layer. We can represent these connections in a $\\mathbb{R}^{50 \\times 25}$ matrix. Each node of the second hidden layer also has a bias term which can be represented in a $\\mathbb{R}^{25}$ matrix. Lastly, each of the 25 nodes in the second hidden layer connects to all of the 10 nodes in the output layer. These weights can be represented in a $\\mathbb{R}^{25\\times 10}$ matrix. Each one of the output nodes also has a bias term which can be represented in a $\\mathbb{R}^{10}$ matrix. Therefore the total number of parameters is \n",
        "$$100 \\times 50 + 50 + 50 \\times 25 + 25 + 25 \\times 10 + 10 = 6585$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Axu9AelNIPvW"
      },
      "source": [
        "### 1.2 Given the loss functions of [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) and [CE (cross-entropy) loss (between logits and target)](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), and the predicted logit of a data example (before softmax) is [0.5, 0.3, 0.8], while the target of a data example is [0.3, 0.6, 0.1], calculate the value of the loss (MSE and CE). (10 points)\n",
        "\n",
        "The loss functions of MSE and CE are as follows,\n",
        "$$\n",
        "\\begin{aligned}\n",
        "  \\ell_{MSE}(\\hat{y}, y) &= \\sum_{i \\in C} (\\hat{y}_c - y_c)^2,\\\\\n",
        "  \\ell_{\\text{CE}}(\\hat{y}, y) &=  -\\sum_{i=1}^{C} y_i \\log\\left(\\frac{\\exp(\\hat{y}_i)}{\\sum_{j \\in [C]}\\exp(\\hat{y}_j)}\\right)\\,,\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $\\hat{y}_i$ is the $i$-th element of predict logit (before softmax), $y$ is the $i$-th element of target, and $C$ is the number of classes.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jQtDx5rcIPvW"
      },
      "source": [
        "Solution:\n",
        "[Your answer here, You should give the full calculation process.]\n",
        "\n",
        "\n",
        "For MSE the calculation is as follows\n",
        "$$\\ell_{MSE}(\\hat{y}, y) = \\sum_{i \\in C}(\\hat{y}_c - y_c)^2 = (0.5 - 0.3)^2 + (0.3 - 0.6)^2 + (0.8 - 0.1)^2 = 0.62$$\n",
        "For the CE calculation \n",
        "\\begin{align*}\n",
        "     \\ell_{CE}(\\hat{y}, y) & = - \\sum_{i = 1}^{C}y_i \\log{\\bigg(\\frac{\\exp{(\\hat{y_i})}}{\\sum_{j \\in [C]} \\exp{(\\hat{y_j})}}\\bigg)} \\\\\n",
        "     & = - \\bigg(0.3\\log{(\\frac{e^{0.5}}{e^{0.5} + e^{0.3} + e^{0.8}})} + 0.6\\log{(\\frac{e^{0.3}}{e^{0.5} + e^{0.3} + e^{0.8}}) + 0.1\\log{(\\frac{e^{0.8}}{e^{0.5} + e^{0.3} + e^{0.8}})}}\\bigg) \\\\\n",
        "     & = -1.24328655533\n",
        "\\end{align*}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W8s1lWXVIPvX"
      },
      "source": [
        "### 1.3 Given a convolutional layer in a CNN with an input feature map of size 32x32, a filter size of 3x3, a stride of 1, and no padding, calculate the size of the output feature map. (10 points)\n",
        "\n",
        "You can refer to [PyTorch Convolutional Layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for the definition of the convolutional layer."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgx-bIK0IPvX"
      },
      "source": [
        "Solution:\n",
        "[Your answer here, You should give the full calculation process.]\n",
        "\n",
        "According to lecture notes, for an input of size $m \\times n \\times c$, filter size of $a \\times b \\times c$, stride of size $s \\times t$ and padding of size $p \\times q$, the output size is $\\lfloor 1 + \\frac{m + 2p - a}{s} \\rfloor \\times \\lfloor 1 + \\frac{n + 2q - b}{t} \\rfloor$. In our example, the padding is 0 therefore $p = q = 0$, the stride is 1 therefore $s = t = 1$, thre input image is $32 \\times 32 \\times 1$ therefore $m = n = 32$, the filter is $3 \\times 3 \\times 1$ therefore $a = b = 3$. The formula yeilds that the output dimension is \n",
        "$$\\lfloor 1 + \\frac{m + 2p - a}{s} \\rfloor \\times \\lfloor 1 + \\frac{n + 2q - b}{t} \\rfloor = \\lfloor 1 + \\frac{32 - 3}{1} \\rfloor \\times \\lfloor 1 + \\frac{32 - 3}{1} \\rfloor = 30 \\times 30$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "njiSliQOIPvX"
      },
      "source": [
        "### 1.4 Given a Transformer encoder with 1 layer, the input and output dimensions of attention of 256, and 8 attention heads, calculate the total number of parameters in the **self-attention mechanism**. (10 points)\n",
        "\n",
        "You can refer to [Attention is all you need (Transformer paper)](https://arxiv.org/abs/1706.03762) for reference."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pcLrJswgIPvY"
      },
      "source": [
        "Solution:\n",
        "[Your answer here, You should give the full calculation process.]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9oe7oy0jwbwu"
      },
      "source": [
        "## Question 2. Implementation of Multi-Layer Perceptron and understanding Gradients (60 points)\n",
        "\n",
        "In this question, you will learn how to implment a Multi-Layer Perceptron (MLP) PyTorch, test the performance on CIFAR10, and learn what is gradient.\n",
        "Please refer to [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) and [PyTorch](https://pytorch.org/) for details.\n",
        "\n",
        "Assuming the MLP follows the following construction\n",
        "$$\n",
        "  \\hat{y} = \\operatorname{softmax}( W_2 \\operatorname{sigmoid}(W_1x + c_1) + c_2)\\,,\n",
        "$$\n",
        "where $\\hat{y}$ is the prediction (probability) of the input $x$ by the MLP and $W_1$, $W_2$, $c_1$, and $c_2$ are four learnable parameters of the MLP. \n",
        "$\\operatorname{softmax}$ and $\\operatorname{sigmoid}$ are the [softmax](https://en.wikipedia.org/wiki/Softmax_function) and [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function.\n",
        "\n",
        "Please note that the label is one-hot vectors, i.e., $y_i$ are either 0 or 1. And the sum of prediction is equal to 1, i.e., $\\sum_{i\\in[C]}\\hat{y}_i = 1$. \n",
        "\n",
        "Tips: The process of using SGD to train a model is as follows:\n",
        "1. Initialize the model parameters.\n",
        "2. Calculate the gradients of the model parameters.\n",
        "3. Update the model parameters using the gradients.\n",
        "4. Test the model.\n",
        "5. Repeat 2 - 4 until the model converges or for a given epochs.\n",
        "\n",
        "You can also refer to [SGD](https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent) and [PyTorch Tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) for inspiration. \n",
        "\n",
        "**Please note that you are allowed to use any PyTorch api and any public code to complete the following coding questions. This assignment is help you to learn PyTorch.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kX9w6dh7IPvY"
      },
      "source": [
        "### 2.0 Get CIFAR10 data (0 points)\n",
        "\n",
        "We will show you how to get the data using PyTorch. **You are not allowed to edit the following code.** \n",
        "Please see [Dataset and DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d-o30eEIPvY"
      },
      "outputs": [],
      "source": [
        "### YOU ARE NOT ALLOWED TO EDIT THE FOLLOWING CODE. ###\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def get_CIFAR10():\n",
        "    # Set the random seed for reproducibility\n",
        "    torch.manual_seed(480)\n",
        "\n",
        "    # Load the CIFAR10 dataset and apply transformations\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, \n",
        "                                     transform=transforms.ToTensor(), \n",
        "                                     download=True)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, \n",
        "                                    transform=transforms.ToTensor())\n",
        "\n",
        "    # Define the data loaders\n",
        "    batch_size = 100\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_dataset, test_dataset, train_loader, test_loader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N6YjwwQFIPvZ"
      },
      "source": [
        "### 2.1 Implement the MLP with PyTorch (15 points)\n",
        "\n",
        "The shape of $W_1$ and $W_2$ should be $3072 \\times 256$ and $256 \\times 10$. \n",
        "\n",
        "You can refer to [Define a NN](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) for inspiration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36NZgKUsIPvZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "##### COMPLETE THE FOLLOWING CODE, YOU ARE ALLOWED TO CHANGE THE PARAMETERS#####\n",
        "\n",
        "# Define the MLP model in PyTorch\n",
        "class MLPinPyTorch(nn.Module):\n",
        "    def __init__(self, input_dim=3072, hidden_dim=256, output_dim=10):\n",
        "        super(MLPinPyTorch, self).__init__()\n",
        "        # TODO: Your code here: init with parameters\n",
        "        #       you are allowed to use any initialization method\n",
        "        \n",
        "        self.hidden1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.output = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.act1 = torch.nn.Sigmoid()\n",
        "        self.act2 = torch.nn.Softmax(dim=1)\n",
        "\n",
        "        # random init\n",
        "        with torch.no_grad():\n",
        "            self.hidden1.weight = torch.nn.Parameter(torch.rand(self.hidden1.weight.shape))\n",
        "            self.hidden1.bias = torch.nn.Parameter(torch.rand(self.hidden1.bias.shape))\n",
        "            self.output.weight = torch.nn.Parameter(torch.rand(self.output.weight.shape))\n",
        "            self.output.bias = torch.nn.Parameter(torch.rand(self.output.bias.shape))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO: Your code here: forward\n",
        "        x = self.hidden1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.output(x)\n",
        "        x = self.act2(x)\n",
        "        return x "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtuen5bgGA94"
      },
      "source": [
        "### 2.2 Calculate the accuracy given true labels and predicted labels (5 points)\n",
        "\n",
        "You should complete the following code, including the calculation of the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrpblVbBGA94"
      },
      "outputs": [],
      "source": [
        "##### COMPLETE THE FOLLOWING CODE, YOU ARE ALLOWED TO CHANGE THE PARAMETERS#####\n",
        "def accuracy(y, predicted):\n",
        "    # TODO: Your code here: calculate accuracy\n",
        "    \"\"\"\n",
        "    y: list of tensors. Each element of the list is a batch of one-hot encoded true label\n",
        "    predicted: list of tensors. Each element of the list is a batch of one-hot encoded predictions \n",
        "    \"\"\"\n",
        "    y = torch.cat(y)\n",
        "    predicted = torch.cat(predicted)\n",
        "    print(y)\n",
        "    print(predicted)\n",
        "    correct = 0\n",
        "\n",
        "    hadamaard = y * predicted ## index i = 1 iff y_i = 1 and hat{y_i} = 1 \n",
        "    example_correct = torch.sum(hadamaard, dim=1)\n",
        "    correct += sum(example_correct)\n",
        "    \n",
        "    return correct / len(y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_SKtKLNSIPva"
      },
      "source": [
        "### 2.3 Test your implementation on CIFAR10 and reports the accuracy on training and testing datasets (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FOpS2fHIPva"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "##### COMPLETE THE FOLLOWING CODE, YOU ARE ALLOWED TO CHANGE THE PARAMETERS#####\n",
        "\n",
        "def test(model, train_dataloader, test_dataloader):\n",
        "    # TODO: Your code here: calculate accuracy\n",
        "    predict_train = []\n",
        "    predict_test = []\n",
        "    train_labels = []\n",
        "    test_labels = []\n",
        "    \n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.view(-1, 3072).to(device)\n",
        "        train_labels.append(labels)\n",
        "        \n",
        "        # Forward pass\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.argmax(predictions, dim=1)\n",
        "        predict_train.append(predictions)\n",
        "        \n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.view(-1, 3072).to(device)\n",
        "        test_labels.append(labels)\n",
        "        \n",
        "        # Forward pass\n",
        "        predictions = model(inputs)\n",
        "        predictions = torch.argmax(predictions, dim=1)\n",
        "        predict_test.append(predictions)\n",
        "        \n",
        "    train_acc = accuracy(train_labels, predict_train)\n",
        "    test_acc = accuracy(test_labels, predict_test)\n",
        "    return train_acc, test_acc\n",
        "\n",
        "###### the following is served for you to check the functions #####\n",
        "##### you can change but ensure the following code can output the accuracies #####\n",
        "model = MLPinPyTorch()\n",
        "train_dataset, test_dataset, train_loader, test_loader = get_CIFAR10()\n",
        "train_acc, test_acc = test(model, train_loader, test_loader)\n",
        "print(f\"train_acc: {float(train_acc.item()):.3f}\")\n",
        "print(f\"test_acc: {float(test_acc.item()): .3f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1vwuOX6dIPva"
      },
      "source": [
        "### 2.4 Calculate the gradients $\\frac{\\partial \\ell}{\\partial W_1}$, $\\frac{\\partial \\ell}{\\partial c_1}$, $\\frac{\\partial \\ell}{\\partial W_2}$, and $\\frac{\\partial \\ell}{\\partial c_2}$ using algebra given a data example $(x, y)$ (20 points)\n",
        "\n",
        "The loss function we use for training the MLP is the [log-loss](https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss), which is defined as follows:\n",
        "$$\n",
        "  \\ell(\\hat{y}, y) = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\\,,\n",
        "$$\n",
        "where $C$ is the number of classes, $y_i$ and $\\hat{y}_i$ are the $i$-th index of $y$ and $\\hat{y}$.\n",
        "\n",
        "Considering the MLP model in question 2, please use chainrule to calculate the gradients. You can directly write LaTex/Markdown in the following cell."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EyP0Yv4zIPva"
      },
      "source": [
        "Solution:\n",
        "[Your answer here]\n",
        "\n",
        "Lets first define some intermediate variables to make the step by step derivation easier. Let\n",
        "$$A = W_1x + c_1$$\n",
        "$$B = W_2 sigmoid(A) + c_2$$\n",
        "Now we can view the MLP as \n",
        "$$\\hat{y} = softmax(B)$$\n",
        "Now we can define the partial derivatives and begin the derivation\n",
        "$$\\frac{\\partial \\ell}{\\partial W_1} = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial B} \\frac{\\partial B}{\\partial A} \\frac{\\partial A}{\\partial W_1}$$\n",
        "$$\\frac{\\partial \\ell}{\\partial c_1} = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial B} \\frac{\\partial B}{\\partial A} \\frac{\\partial A}{\\partial c_1}$$\n",
        "$$\\frac{\\partial \\ell}{\\partial W_2} = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial B} \\frac{\\partial B}{\\partial W_2}$$\n",
        "$$\\frac{\\partial \\ell}{\\partial W_2} = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial B} \\frac{\\partial B}{\\partial c_2}$$\n",
        "We notice that all of the required derivatives begin with $\\frac{\\partial \\ell}{\\partial \\hat{y}}$. Since $\\hat{y}$ is a vector the gradient of the loss with respect to $\\hat{y}$ is also a vector given by \n",
        "$$\\frac{\\partial \\ell}{\\partial \\hat{y}} = \\begin{bmatrix} \\frac{\\partial \\ell}{\\partial \\hat{y}_1} & \\frac{\\partial \\ell}{\\partial \\hat{y}_2} & ... & \\frac{\\partial \\ell}{\\partial \\hat{y}_C}\\end{bmatrix} = \\begin{bmatrix}\\frac{-y_1}{\\hat{y}_1} & \\frac{-y_2}{\\hat{y}_2} & ... & \\frac{-y_C}{\\hat{y}_C}\\end{bmatrix}$$\n",
        "Now $\\frac{\\partial \\hat{y}}{\\partial B}$. Notice that $\\hat{y}$ is a probability vector therefore the gradient should be \n",
        "$$\\frac{\\partial \\hat{y}}{\\partial B} = \\begin{bmatrix}\\frac{\\partial \\hat{y}_1}{\\partial B} & \\frac{\\partial \\hat{y}_2}{\\partial B} & ... & \\frac{\\partial \\hat{y}_C}{\\partial B}\\end{bmatrix}$$ \n",
        "\n",
        "However, $B$ itself is a vector with C dimensions. Therefore the gradient $\\frac{\\partial \\hat{y}}{\\partial B}$ is a Jacobian\n",
        "\\begin{align}\n",
        "    \\frac{\\partial \\hat{y}}{\\partial B} & = \\begin{bmatrix} \\frac{\\partial \\hat{y}_1}{\\partial B_1} & \\frac{\\partial \\hat{y}_1}{\\partial B_2} & \\cdots & \\frac{\\partial \\hat{y}_1}{\\partial B_C} \\\\\n",
        "    \\frac{\\partial \\hat{y}_2}{\\partial B_1} & \\frac{\\partial \\hat{y}_2}{\\partial B_2} & \\cdots & \\frac{\\partial \\hat{y}_2}{\\partial B_C} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    \\frac{\\partial \\hat{y}_C}{\\partial B_1} & \\frac{\\partial \\hat{y}_C}{\\partial B_2} & \\cdots & \\frac{\\partial \\hat{y}_C}{\\partial B_C}\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        \\frac{e^{B_1}(\\sum_{i=1}^Ce^{B_i}) - e^{B_1}e^{B_1}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\frac{0 \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_1}e^{B_2}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\cdots & \\frac{0 \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_1}e^{B_C}}{(\\sum_{i=1}^Ce^{B_i})^2} \\\\\n",
        "        \\frac{0 \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_2}e^{B_1}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\frac{e^{B_2} \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_2}e^{B_2}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\cdots & \\frac{0 \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_2}e^{B_C}}{(\\sum_{i=1}^Ce^{B_i})^2} \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        \\frac{0 \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_C}e^{B_1}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\frac{0 \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_C}e^{B_2}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\cdots & \\frac{e^{B_C} \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_C}e^{B_C}}{(\\sum_{i=1}^Ce^{B_i})^2} \\\\\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        \\frac{e^{B_1}(\\sum_{i=1}^Ce^{B_i}) - e^{B_1}e^{B_1}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\frac{- e^{B_1}e^{B_2}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\cdots & \\frac{- e^{B_1}e^{B_C}}{(\\sum_{i=1}^Ce^{B_i})^2} \\\\\n",
        "        \\frac{- e^{B_2}e^{B_1}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\frac{e^{B_2} \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_2}e^{B_2}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\cdots & \\frac{- e^{B_2}e^{B_C}}{(\\sum_{i=1}^Ce^{B_i})^2} \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        \\frac{- e^{B_C}e^{B_1}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\frac{- e^{B_C}e^{B_2}}{(\\sum_{i=1}^Ce^{B_i})^2} & \\cdots & \\frac{e^{B_C} \\times (\\sum_{i=1}^Ce^{B_i}) - e^{B_C}e^{B_C}}{(\\sum_{i=1}^Ce^{B_i})^2} \\\\\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        \\frac{e^{B_1}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{\\sum_{i=1}^Ce^{B_i} - e^{B_1}}{\\sum_{i=1}^Ce^{B_i}} & \\frac{- e^{B_1}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{e^{B_2}}{\\sum_{i=1}^Ce^{B_i}} & \\cdots & \\frac{- e^{B_1}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{e^{B_C}}{\\sum_{i=1}^Ce^{B_i}} \\\\\n",
        "        \\frac{- e^{B_2}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{e^{B_1}}{\\sum_{i=1}^Ce^{B_i}} & \\frac{e^{B_2}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{ \\sum_{i=1}^Ce^{B_i} - e^{B_2}}{\\sum_{i=1}^Ce^{B_i}} & \\cdots & \\frac{- e^{B_2}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{e^{B_C}}{\\sum_{i=1}^Ce^{B_i}} \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        \\frac{- e^{B_C}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{e^{B_1}}{\\sum_{i=1}^Ce^{B_i}} & \\frac{- e^{B_C}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{e^{B_2}}{\\sum_{i=1}^Ce^{B_i}} & \\cdots & \\frac{e^{B_C}}{\\sum_{i=1}^Ce^{B_i}} \\times \\frac{\\sum_{i=1}^Ce^{B_i} - e^{B_C}}{\\sum_{i=1}^Ce^{B_i}} \\\\\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        \\hat{y}_1 \\times (1 - \\hat{y}_1) & -\\hat{y}_1 \\times \\hat{y}_2 & \\cdots & -\\hat{y}_1 \\times \\hat{y}_C \\\\\n",
        "        -\\hat{y}_2 \\times \\hat{y}_1 & \\hat{y}_2 \\times (1 - \\hat{y}_2) & \\cdots & -\\hat{y}_2 \\times \\hat{y}_C \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        -\\hat{y}_C \\times \\hat{y}_1 & -\\hat{y}_C \\times \\hat{y}_{2} & \\cdots & \\hat{y}_C \\times (1 - \\hat{y}_C)\n",
        "    \\end{bmatrix}\n",
        "\\end{align}\n",
        "Now to find \n",
        "\\begin{align}\n",
        "    \\frac{\\partial \\ell}{\\partial B} & = \\frac{\\partial \\ell}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial B} \\\\\n",
        "    & = \\begin{bmatrix}\\frac{-y_1}{\\hat{y}_1} & \\frac{-y_2}{\\hat{y}_2} & ... & \\frac{-y_C}{\\hat{y}_C}\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
        "        \\hat{y}_1 \\times (1 - \\hat{y}_1) & -\\hat{y}_1 \\times \\hat{y}_2 & \\cdots & -\\hat{y}_1 \\times \\hat{y}_C \\\\\n",
        "        -\\hat{y}_2 \\times \\hat{y}_1 & \\hat{y}_2 \\times (1 - \\hat{y}_2) & \\cdots & -\\hat{y}_2 \\times \\hat{y}_C \\\\\n",
        "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "        -\\hat{y}_C \\times \\hat{y}_1 & -\\hat{y}_C \\times \\hat{y}_{2} & \\cdots & \\hat{y}_C \\times (1 - \\hat{y}_C)\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        -y_1 (1 - \\hat{y}_1) + \\sum_{i=1, i \\ne 1}^{C} y_i\\hat{y}_1 & -y_2 (1 - \\hat{y}_2) + \\sum_{i=1, i \\ne 2}^{C}  y_i\\hat{y}_2 & \\cdots & -y_C (1 - \\hat{y}_C) + \\sum_{i=1, i \\ne C}^{C} y_i\\hat{y}_C\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        -y_1 + y_1\\hat{y}_1 + \\sum_{i=1, i \\ne 1}^{C} y_i\\hat{y}_1 & -y_2 + y_2\\hat{y}_2 + \\sum_{i=1, i \\ne 2}^{C}  y_i\\hat{y}_2 & \\cdots & -y_C + y_C\\hat{y}_C + \\sum_{i=1, i \\ne C}^{C} y_i\\hat{y}_C \n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        -y_1 + \\sum_{i=1}^{C} y_i\\hat{y}_1 & -y_2 + \\sum_{i=1}^{C}  y_i\\hat{y}_2 & \\cdots & -y_C + \\sum_{i=1}^{C} y_i\\hat{y}_C\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        -y_1 + \\hat{y}_1\\sum_{i=1}^{C} y_i & -y_2 + \\hat{y}_2\\sum_{i=1}^{C}  y_i & \\cdots & -y_C + \\hat{y}_C\\sum_{i=1}^{C} y_i\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\begin{bmatrix}\n",
        "        -y_1 + \\hat{y}_1 & -y_2 + \\hat{y}_2 & \\cdots & -y_C + \\hat{y}_C\n",
        "    \\end{bmatrix} \\\\\n",
        "    & = \\hat{\\textbf{y}} - \\textbf{y}\n",
        "\\end{align}\n",
        "Now we find $\\frac{\\partial B}{\\partial A}$\n",
        "\\begin{align}\n",
        "    \\frac{\\partial B}{\\partial A} & = W_2 \\times sigmoid'(A)\n",
        "\\end{align}\n",
        "where $sigmoid'(A) = sigmoid(A) \\times (1 - sigmoid(A))$\n",
        "Now\n",
        "$$\\frac{\\partial A}{\\partial W_1} = x$$\n",
        "and \n",
        "$$\\frac{\\partial A}{\\partial c_1} = \\vec{1}$$\n",
        "We can also find that \n",
        "$$\\frac{\\partial B}{\\partial W_2} = sigmoid(A)$$\n",
        "and \n",
        "$$\\frac{\\partial B}{ \\partial c_2} = \\vec{1}$$\n",
        "Putting it all together we get that\n",
        "$$\\frac{\\partial \\ell}{\\partial W_1} = (\\hat{\\textbf{y}} - \\textbf{y}) \\times W_2 \\times sigmoid'(A) \\times x$$\n",
        "$$\\frac{\\partial \\ell}{\\partial c_1} = (\\hat{\\textbf{y}} - \\textbf{y}) \\times W_2 \\times sigmoid'(A)$$\n",
        "$$\\frac{\\partial \\ell}{\\partial W_2} = (\\hat{\\textbf{y}} - \\textbf{y}) \\times sigmoid(A)$$\n",
        "$$\\frac{\\partial \\ell}{\\partial c_2} =  (\\hat{\\textbf{y}} - \\textbf{y})$$"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('cs479')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "9b9c6d00f578c9eeab1a200e0145ca4c071afb716f83c3c45206482f48392ae3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
